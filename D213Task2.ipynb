{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVMzCvI/gGxgnXm1pyqWo9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salad-distribution/R-project-showcase/blob/main/D213Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-jzXQVZLQTl"
      },
      "outputs": [],
      "source": [
        "setwd ('C:/Users/julia/OneDrive/Desktop/WGU/213 Advanced Data Analytics')\n",
        "library('tidyverse')\n",
        "churn <- read.csv('teleco_time_series.csv', row.names = \"Day\") ## Day column assigned to index values\n",
        "churn_series <- read.csv('teleco_time_series.csv') ## load data with original \"Day\" column included\n",
        "TimeSeries <- ts(churn_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part II: Data Preparation**"
      ],
      "metadata": {
        "id": "I3yY5eZDMTQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B.  Summarize the data cleaning process by doing the following:\n",
        "\n",
        "1.  Perform exploratory data analysis on the chose data set, and include an explanation of each of the following elements:\n",
        "\n",
        "      *   presence of unusual characters (e.g., emojis, non-English characters)\n",
        "      *   vocabulary size\n",
        "      *   proposed word embedding length\n",
        "      *   statistical justification for the chosen maximum sequence length\n",
        "\n",
        "2.  Describe the goals of the tokenization process, including any code generated and packages that are used to normalize text during the tokenization process.\n"
      ],
      "metadata": {
        "id": "kPub-UrQMRLW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWCe8SLiL0Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Explain the padding process used to standardize the length of sequences. Include the following in your explanation:\n",
        "\n",
        "      *   if the padding occurs before or after the text sequence\n",
        "      *   a screenshot of a single padded sequence\n",
        "\n",
        "4.  Identify how many categories of sentiment will be used and an activation function for the final dense layer of the network."
      ],
      "metadata": {
        "id": "NdrBgDWtNYbN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kerQDVOfNky2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Explain the steps used to prepare the data for analysis, including the size of the training, validation, and test set split (based on the industry average)."
      ],
      "metadata": {
        "id": "RV94SziRNlu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## last 20% of lags set to test data, first 80% of lags set to training data\n",
        "library(TSstudio)\n",
        "split <- ts_split(diff_churn, sample.out = 146)\n",
        "rownames(split$test) <- c(585:730)\n",
        "train <- split$train\n",
        "test <-  split$test\n",
        "splitNEW <- ts_split(ts_churn, sample.out = 146)\n",
        "rownames(splitNEW$test) <- c(586:731)\n",
        "trainNEW <- splitNEW$train\n",
        "testNEW <- splitNEW$test"
      ],
      "metadata": {
        "id": "T2H5V1IwNpaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Provide a copy of the cleaned data set."
      ],
      "metadata": {
        "id": "A7ECD4wTNtw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write.csv(train, 'D213diffTrainC5.csv')\n",
        "write.csv(test, 'D213diffTestC5.csv')\n",
        "write.csv(trainNEW, 'D213TrainC5.csv')\n",
        "write.csv(testNEW, 'D213TestC5.csv')"
      ],
      "metadata": {
        "id": "uKkB6JB9Nv8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part III: Network Architecture**"
      ],
      "metadata": {
        "id": "WALNlcXjNyz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Describe the type of neural network used by doing the following:\n",
        "\n",
        "1. Provide the output of the model summary of the function from TensorFlow.\n",
        "\n",
        "2. Discuss the number of layers, the type of layers, and the total number of parameters.\n",
        "\n",
        "3. Justify the choice of hyperparameters, including the following elements:\n",
        "\n",
        "      *   activation functions\n",
        "      *   number of nodes per layer\n",
        "      *   loss function\n",
        "      *   optimizer\n",
        "      *   stopping criteria\n",
        "      *   evaluation metric"
      ],
      "metadata": {
        "id": "iot80lE7OAlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part IV: Model Evaluation**"
      ],
      "metadata": {
        "id": "BYKmhzKAORsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D. Evaluate the model training process and its relevant outcomes by doing the following:\n",
        "\n",
        "1. Discuss the impact of using stopping criteria to include defining the number of epochs, including a screenshot showing the final training epoch.\n",
        "\n",
        "2. Assess the fitness of the model and any actions taken to address overfitting.\n",
        "\n",
        "    ->> run on rerun of KERNEL each time otherwise model will be re-instantiated on trained data, which can lead to overfitting. Don't train training data until reaching 100% - Barrel\n",
        "\n",
        "3. Provide visualizations of the model's training process, including a line graph of the loss and chosen evaluation metric.\n",
        "\n",
        "2. Discuss the predictive accuracy of the trained network using the chosen evaluation metric from part D3."
      ],
      "metadata": {
        "id": "Yhpc7Ur9OY4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part V: Summary and Recommendations**"
      ],
      "metadata": {
        "id": "qhQouZ4tO3RO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E. Provide the code you used to save the trained network within the neural network.\n",
        "\n",
        "See above or see paper submitted\n",
        "\n",
        "F. Discuss the functionality of your neural network, including the impact of the network architecture.\n",
        "\n",
        "G. Recommend a course of action based on your results."
      ],
      "metadata": {
        "id": "6pJhZLnlO-rI"
      }
    }
  ]
}